#!/usr/bin/env python3
"""
æ¸…ç†åŒ…å«éžçº¯å­—æ¯å•è¯çš„å…³å¡ï¼Œå¹¶é‡æ–°ç¼–å·ä½¿å…³å¡è¿žç»­

åŠŸèƒ½ï¼š
1. æ‰«ææ‰€æœ‰è¯åº“çš„å…³å¡æ–‡ä»¶
2. æ‰¾å‡ºåŒ…å«éžçº¯å­—æ¯å•è¯ï¼ˆå¦‚è¿žå­—ç¬¦ã€æ’‡å·ã€ç©ºæ ¼ç­‰ï¼‰çš„å…³å¡
3. åˆ é™¤è¿™äº›å…³å¡
4. é‡æ–°ç¼–å·å‰©ä½™å…³å¡ï¼Œç¡®ä¿ç¼–å·è¿žç»­ï¼ˆä¸è·³å·ï¼‰
5. æ›´æ–° meta.json ä¸­çš„å…³å¡æ•°é‡
"""

import json
import os
import shutil
from pathlib import Path
from typing import List, Dict, Set


def is_pure_alpha(word: str) -> bool:
    """æ£€æŸ¥å•è¯æ˜¯å¦åªåŒ…å«26ä¸ªè‹±æ–‡å­—æ¯"""
    return word.isalpha()


def check_level_validity(level_path: Path) -> tuple[bool, List[str]]:
    """æ£€æŸ¥å…³å¡æ˜¯å¦æœ‰æ•ˆï¼ˆæ‰€æœ‰å•è¯éƒ½æ˜¯çº¯å­—æ¯ï¼‰
    
    è¿”å›ž: (æ˜¯å¦æœ‰æ•ˆ, æ— æ•ˆå•è¯åˆ—è¡¨)
    """
    try:
        with open(level_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        invalid_words = []
        for word_info in data.get('words', []):
            word = word_info.get('word', '')
            if not is_pure_alpha(word):
                invalid_words.append(word)
        
        return len(invalid_words) == 0, invalid_words
    except Exception as e:
        print(f"  è¯»å–å…³å¡æ–‡ä»¶å‡ºé”™ {level_path}: {e}")
        return False, ["ERROR"]


def get_all_level_files(group_dir: Path) -> List[Path]:
    """èŽ·å–è¯åº“ç›®å½•ä¸‹æ‰€æœ‰å…³å¡æ–‡ä»¶ï¼ˆæŒ‰ç¼–å·æŽ’åºï¼‰"""
    level_files = []
    for f in group_dir.iterdir():
        if f.suffix == '.json' and f.stem.isdigit():
            level_files.append(f)
    level_files.sort(key=lambda x: int(x.stem))
    return level_files


def cleanup_group(group_dir: Path, dry_run: bool = False) -> Dict:
    """æ¸…ç†å•ä¸ªè¯åº“çš„æ— æ•ˆå…³å¡
    
    è¿”å›žç»Ÿè®¡ä¿¡æ¯
    """
    group_name = group_dir.name
    level_files = get_all_level_files(group_dir)
    
    if not level_files:
        return {"group": group_name, "total": 0, "invalid": 0, "deleted": []}
    
    # æ£€æŸ¥æ¯ä¸ªå…³å¡
    valid_levels = []
    invalid_levels = []
    
    for level_path in level_files:
        is_valid, invalid_words = check_level_validity(level_path)
        if is_valid:
            valid_levels.append(level_path)
        else:
            invalid_levels.append({
                "path": level_path,
                "level": int(level_path.stem),
                "invalid_words": invalid_words
            })
    
    if not invalid_levels:
        return {
            "group": group_name,
            "total": len(level_files),
            "invalid": 0,
            "deleted": [],
            "final_count": len(level_files)
        }
    
    # æ‰“å°æ— æ•ˆå…³å¡ä¿¡æ¯
    print(f"\nðŸ“‚ {group_name}: å‘çŽ° {len(invalid_levels)} ä¸ªæ— æ•ˆå…³å¡")
    for inv in invalid_levels:
        print(f"   ç¬¬ {inv['level']} å…³: {', '.join(inv['invalid_words'])}")
    
    if dry_run:
        return {
            "group": group_name,
            "total": len(level_files),
            "invalid": len(invalid_levels),
            "deleted": [inv["level"] for inv in invalid_levels],
            "final_count": len(valid_levels)
        }
    
    # åˆ é™¤æ— æ•ˆå…³å¡å¹¶é‡æ–°ç¼–å·
    print(f"   æ­£åœ¨åˆ é™¤å¹¶é‡æ–°ç¼–å·...")
    
    # å…ˆåˆ é™¤æ‰€æœ‰æ— æ•ˆå…³å¡
    for inv in invalid_levels:
        os.remove(inv["path"])
    
    # èŽ·å–å‰©ä½™çš„æœ‰æ•ˆå…³å¡ï¼ˆæŒ‰åŽŸç¼–å·æŽ’åºï¼‰
    remaining_files = get_all_level_files(group_dir)
    
    # é‡æ–°ç¼–å·
    for new_num, level_path in enumerate(remaining_files, start=1):
        old_num = int(level_path.stem)
        if old_num != new_num:
            # éœ€è¦é‡å‘½å
            new_path = level_path.parent / f"{new_num}.json"
            # åŒæ—¶æ›´æ–°æ–‡ä»¶å†…å®¹ä¸­çš„ level å­—æ®µ
            with open(level_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            data['level'] = new_num
            with open(level_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
            # é‡å‘½åæ–‡ä»¶
            shutil.move(level_path, new_path)
    
    # æ›´æ–° meta.json
    meta_path = group_dir / 'meta.json'
    if meta_path.exists():
        with open(meta_path, 'r', encoding='utf-8') as f:
            meta = json.load(f)
        old_count = meta.get('level_count', len(level_files))
        meta['level_count'] = len(valid_levels)
        meta['success_count'] = len(valid_levels)
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        print(f"   æ›´æ–° meta.json: {old_count} -> {len(valid_levels)} å…³")
    
    print(f"   âœ… å®Œæˆ: åˆ é™¤ {len(invalid_levels)} å…³ï¼Œå‰©ä½™ {len(valid_levels)} å…³")
    
    return {
        "group": group_name,
        "total": len(level_files),
        "invalid": len(invalid_levels),
        "deleted": [inv["level"] for inv in invalid_levels],
        "final_count": len(valid_levels)
    }


def main():
    import argparse
    parser = argparse.ArgumentParser(description="æ¸…ç†æ— æ•ˆå…³å¡å¹¶é‡æ–°ç¼–å·")
    parser.add_argument('--dry-run', action='store_true', help='åªæ‰«æä¸æ‰§è¡Œåˆ é™¤')
    parser.add_argument('-g', '--groups', nargs='+', help='æŒ‡å®šè¯åº“ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰')
    args = parser.parse_args()
    
    levels_dir = Path(__file__).parent.parent / 'data' / 'levels'
    
    if not levels_dir.exists():
        print(f"âŒ å…³å¡ç›®å½•ä¸å­˜åœ¨: {levels_dir}")
        return
    
    # èŽ·å–æ‰€æœ‰è¯åº“ç›®å½•
    if args.groups:
        group_dirs = [levels_dir / g for g in args.groups if (levels_dir / g).is_dir()]
    else:
        group_dirs = [d for d in levels_dir.iterdir() if d.is_dir()]
    
    group_dirs.sort(key=lambda x: x.name)
    
    print(f"{'=' * 60}")
    print(f"æ¸…ç†æ— æ•ˆå…³å¡ï¼ˆåŒ…å«éžçº¯å­—æ¯å•è¯ï¼‰")
    print(f"{'=' * 60}")
    print(f"æ¨¡å¼: {'æ‰«ææ¨¡å¼ï¼ˆä¸åˆ é™¤ï¼‰' if args.dry_run else 'æ‰§è¡Œæ¨¡å¼ï¼ˆä¼šåˆ é™¤å¹¶é‡ç¼–å·ï¼‰'}")
    print(f"è¯åº“æ•°é‡: {len(group_dirs)}")
    
    # å¤„ç†æ¯ä¸ªè¯åº“
    results = []
    total_invalid = 0
    total_deleted = 0
    
    for group_dir in group_dirs:
        result = cleanup_group(group_dir, args.dry_run)
        results.append(result)
        total_invalid += result["invalid"]
        if not args.dry_run:
            total_deleted += result["invalid"]
    
    # æ±‡æ€»æŠ¥å‘Š
    print(f"\n{'=' * 60}")
    print(f"æ±‡æ€»æŠ¥å‘Š")
    print(f"{'=' * 60}")
    
    affected_groups = [r for r in results if r["invalid"] > 0]
    if affected_groups:
        print(f"\nå—å½±å“çš„è¯åº“ ({len(affected_groups)} ä¸ª):")
        for r in affected_groups:
            deleted_str = ', '.join(map(str, r["deleted"][:10]))
            if len(r["deleted"]) > 10:
                deleted_str += f" ... ç­‰ {len(r['deleted'])} å…³"
            print(f"  {r['group']}: åˆ é™¤ {r['invalid']} å…³ ({r['total']} -> {r['final_count']})")
            print(f"    åˆ é™¤çš„å…³å¡: {deleted_str}")
    else:
        print("\nâœ… æ‰€æœ‰å…³å¡éƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œæ— éœ€æ¸…ç†")
    
    print(f"\næ€»è®¡:")
    print(f"  æ‰«æè¯åº“: {len(results)} ä¸ª")
    print(f"  æ— æ•ˆå…³å¡: {total_invalid} ä¸ª")
    if not args.dry_run:
        print(f"  å·²åˆ é™¤å¹¶é‡ç¼–å·: {total_deleted} ä¸ª")
    
    # æ›´æ–° levels_summary.json
    if not args.dry_run and total_deleted > 0:
        summary_path = levels_dir / 'levels_summary.json'
        if summary_path.exists():
            print("\næ­£åœ¨æ›´æ–° levels_summary.json...")
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            for r in results:
                if r["invalid"] > 0 and r["group"] in summary.get("groups", {}):
                    summary["groups"][r["group"]]["level_count"] = r["final_count"]
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print("âœ… levels_summary.json å·²æ›´æ–°")


if __name__ == "__main__":
    main()
