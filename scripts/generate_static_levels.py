#!/usr/bin/env python3
"""
ç”Ÿæˆé™æ€å…³å¡æ•°æ®æ–‡ä»¶ - éšå‰ç«¯åˆ†å‘

æ ¹æ®è®¾è®¡ç®€æŠ¥ç”Ÿæˆå„åˆ†ç±»çš„é—¯å…³å…³å¡æ•°æ®ï¼š
- 4Ã—4: 9å…³
- 5Ã—5: 18å…³  
- 6Ã—6: 18å…³
- 7Ã—7: 18å…³
- 8Ã—8: å°å­¦54å…³ï¼Œå…¶ä»–18å…³
- 9Ã—9: 18å…³ï¼ˆå°å­¦æ— ï¼‰
- 10Ã—10: 81å…³ï¼ˆå°å­¦æ— ï¼‰

å°å­¦æ€»å…±117å…³ï¼Œå…¶ä»–è¯åº“180å…³
"""
import os
import sys
import json
import random
import time
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass, field, asdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "src" / "backend"))

from csp_puzzle_generator import CSPPuzzleGenerator


# å…³å¡é…ç½®
LEVEL_CONFIG = {
    "primary": {  # å°å­¦ - 117å…³
        "4x4": 9,
        "5x5": 18,
        "6x6": 18,
        "7x7": 18,
        "8x8": 54,  # å°å­¦8x8æœ‰54å…³ä½œä¸ºç»ˆææŒ‘æˆ˜
        # å°å­¦æ²¡æœ‰9x9å’Œ10x10
    },
    "other": {  # å…¶ä»–è¯åº“ - 180å…³
        "4x4": 9,
        "5x5": 18,
        "6x6": 18,
        "7x7": 18,
        "8x8": 18,
        "9x9": 18,
        "10x10": 81,
    }
}


# è¯åº“åˆ†ç±»æ˜ å°„
VOCABULARY_MAPPING = {
    # å°å­¦ï¼ˆäººæ•™ç‰ˆPEPï¼‰
    "primary": {
        "grade3_1": {"book_id": "PEPXiaoXue3_1", "name": "ä¸‰å¹´çº§ä¸Šå†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade3_2": {"book_id": "PEPXiaoXue3_2", "name": "ä¸‰å¹´çº§ä¸‹å†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade4_1": {"book_id": "PEPXiaoXue4_1", "name": "å››å¹´çº§ä¸Šå†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade4_2": {"book_id": "PEPXiaoXue4_2", "name": "å››å¹´çº§ä¸‹å†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade5_1": {"book_id": "PEPXiaoXue5_1", "name": "äº”å¹´çº§ä¸Šå†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade5_2": {"book_id": "PEPXiaoXue5_2", "name": "äº”å¹´çº§ä¸‹å†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade6_1": {"book_id": "PEPXiaoXue6_1", "name": "å…­å¹´çº§ä¸Šå†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
        "grade6_2": {"book_id": "PEPXiaoXue6_2", "name": "å…­å¹´çº§ä¸‹å†Œ", "category": "04_äººæ•™ç‰ˆå°å­¦"},
    },
    # åˆä¸­ï¼ˆäººæ•™ç‰ˆï¼‰
    "junior": {
        "junior7_1": {"book_id": "PEPChuZhong7_1", "name": "ä¸ƒå¹´çº§ä¸Šå†Œ", "category": "05_äººæ•™ç‰ˆåˆä¸­"},
        "junior7_2": {"book_id": "PEPChuZhong7_2", "name": "ä¸ƒå¹´çº§ä¸‹å†Œ", "category": "05_äººæ•™ç‰ˆåˆä¸­"},
        "junior8_1": {"book_id": "PEPChuZhong8_1", "name": "å…«å¹´çº§ä¸Šå†Œ", "category": "05_äººæ•™ç‰ˆåˆä¸­"},
        "junior8_2": {"book_id": "PEPChuZhong8_2", "name": "å…«å¹´çº§ä¸‹å†Œ", "category": "05_äººæ•™ç‰ˆåˆä¸­"},
        "junior9": {"book_id": "PEPChuZhong9_1", "name": "ä¹å¹´çº§å…¨å†Œ", "category": "05_äººæ•™ç‰ˆåˆä¸­"},
    },
    # é«˜ä¸­ï¼ˆäººæ•™ç‰ˆï¼‰
    "senior": {
        "senior1": {"book_id": "PEPGaoZhong_1", "name": "é«˜ä¸€å¿…ä¿®1", "category": "07_äººæ•™ç‰ˆé«˜ä¸­"},
        "senior2": {"book_id": "PEPGaoZhong_3", "name": "é«˜äºŒå¿…ä¿®3", "category": "07_äººæ•™ç‰ˆé«˜ä¸­"},
        "senior3": {"book_id": "PEPGaoZhong_5", "name": "é«˜ä¸‰å¿…ä¿®5", "category": "07_äººæ•™ç‰ˆé«˜ä¸­"},
    },
    # è€ƒè¯•ç±»
    "cet4": {"book_id": "CET4_3", "name": "å¤§å­¦å››çº§", "category": "01_è€ƒè¯•ç±»"},
    "cet6": {"book_id": "CET6_3", "name": "å¤§å­¦å…­çº§", "category": "01_è€ƒè¯•ç±»"},
    "postgrad": {"book_id": "KaoYan_3", "name": "è€ƒç ”è¯æ±‡", "category": "11_æ–°ä¸œæ–¹æ‰©å±•"},
    # å‡ºå›½ç•™å­¦
    "ielts": {"book_id": "IELTS_3", "name": "é›…æ€è¯æ±‡", "category": "11_æ–°ä¸œæ–¹æ‰©å±•"},
    "toefl": {"book_id": "TOEFL_3", "name": "æ‰˜ç¦è¯æ±‡", "category": "11_æ–°ä¸œæ–¹æ‰©å±•"},
    "gre": {"book_id": "GRE_3", "name": "GREè¯æ±‡", "category": "11_æ–°ä¸œæ–¹æ‰©å±•"},
    # ä¸­é«˜è€ƒ
    "zhongkao": {"book_id": "ChuZhong_3", "name": "ä¸­è€ƒè¯æ±‡", "category": "03_ä¸­é«˜è€ƒ"},
    "gaokao": {"book_id": "GaoZhong_3", "name": "é«˜è€ƒè¯æ±‡", "category": "03_ä¸­é«˜è€ƒ"},
}


def load_vocabulary(book_id: str, category: str, data_dir: Path) -> List[Dict]:
    """åŠ è½½è¯åº“æ–‡ä»¶"""
    vocab_file = data_dir / "words" / category / f"{book_id}.json"
    if not vocab_file.exists():
        print(f"è­¦å‘Š: è¯åº“æ–‡ä»¶ä¸å­˜åœ¨ {vocab_file}")
        return []
    
    with open(vocab_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    words = []
    for w in data.get("words", []):
        word = w.get("word", "").strip().upper()
        # è¿‡æ»¤æ— æ•ˆå•è¯
        if not word or len(word) < 2 or len(word) > 10:
            continue
        if not word.isalpha():
            continue
        
        # è·å–é‡Šä¹‰
        trans_list = w.get("trans", [])
        definition = ""
        if trans_list:
            t = trans_list[0]
            pos = t.get("pos", "")
            tranCn = t.get("tranCn", "")
            definition = f"{pos} {tranCn}".strip() if pos else tranCn
        
        words.append({
            "word": word,
            "definition": definition or "æ— é‡Šä¹‰",
            "phonetic": w.get("usphone", "") or w.get("ukphone", ""),
        })
    
    return words


def generate_level(generator: CSPPuzzleGenerator, words: List[Dict], grid_size: int, 
                   level_num: int, max_attempts: int = 5) -> Optional[Dict]:
    """ç”Ÿæˆå•ä¸ªå…³å¡"""
    for attempt in range(max_attempts):
        try:
            # æ ¹æ®ç½‘æ ¼å¤§å°è¿‡æ»¤å•è¯
            min_len = max(2, grid_size - 3)
            max_len = grid_size
            
            filtered_words = [w for w in words if min_len <= len(w["word"]) <= max_len]
            if len(filtered_words) < 6:
                # æ”¾å®½é™åˆ¶
                filtered_words = [w for w in words if 2 <= len(w["word"]) <= grid_size]
            
            if len(filtered_words) < 4:
                print(f"  è­¦å‘Š: å…³å¡{level_num} å¯ç”¨å•è¯ä¸è¶³")
                return None
            
            # éšæœºæ‰“ä¹±
            random.shuffle(filtered_words)
            
            # ç”Ÿæˆè°œé¢˜
            puzzle = generator.generate_dense_puzzle(
                grid_size=grid_size,
                vocabulary=filtered_words[:100],  # é™åˆ¶è¯åº“å¤§å°æé«˜é€Ÿåº¦
                min_density=0.35,
                timeout=10.0
            )
            
            if puzzle and puzzle.calculate_density() >= 0.35:
                # è®¡ç®—é¢„å¡«å­—æ¯
                puzzle.compute_revealed_letters(min_reveal=2)
                
                # è½¬æ¢ä¸ºå…³å¡æ•°æ®æ ¼å¼
                level_data = convert_puzzle_to_level(puzzle, level_num, grid_size)
                return level_data
                
        except Exception as e:
            print(f"  å°è¯•{attempt+1}å¤±è´¥: {e}")
            continue
    
    return None


def convert_puzzle_to_level(puzzle, level_num: int, grid_size: int) -> Dict:
    """å°†è°œé¢˜è½¬æ¢ä¸ºå…³å¡æ•°æ®æ ¼å¼"""
    # æ„å»ºcellsçŸ©é˜µ
    cells = [[None for _ in range(grid_size)] for _ in range(grid_size)]
    for r in range(grid_size):
        for c in range(grid_size):
            if puzzle.grid[r][c]:
                cells[r][c] = puzzle.grid[r][c]
    
    # åˆå¹¶æ‰€æœ‰å•è¯
    all_words = []
    word_id = 1
    
    # è®¡ç®—çº¿ç´¢ç¼–å·ï¼ˆå¡«å­—æ¸¸æˆæ ‡å‡†ç¼–å·ï¼‰
    clue_numbers = [[0] * grid_size for _ in range(grid_size)]
    clue_counter = 1
    
    # æ‰¾å‡ºæ‰€æœ‰å•è¯èµ·å§‹ä½ç½®
    word_starts = set()
    for word_info in puzzle.row_words:
        word_starts.add((word_info['row'], word_info['col']))
    for word_info in puzzle.col_words:
        word_starts.add((word_info['row'], word_info['col']))
    
    # æŒ‰ä½ç½®æ’åºåˆ†é…ç¼–å·
    sorted_starts = sorted(word_starts, key=lambda x: (x[0], x[1]))
    for row, col in sorted_starts:
        clue_numbers[row][col] = clue_counter
        clue_counter += 1
    
    # å¤„ç†æ¨ªå‘å•è¯
    for word_info in puzzle.row_words:
        row = word_info['row']
        col = word_info['col']
        word = word_info['word']
        definition = word_info.get('definition', '')
        
        all_words.append({
            "id": word_id,
            "word": word.upper(),
            "definition": definition,
            "direction": "across",
            "start_row": row,
            "start_col": col,
            "length": len(word),
            "clue_number": clue_numbers[row][col],
            "alternatives": []  # å¤‡é€‰ç­”æ¡ˆï¼Œå¯åç»­æ‰©å±•
        })
        word_id += 1
    
    # å¤„ç†çºµå‘å•è¯
    for word_info in puzzle.col_words:
        row = word_info['row']
        col = word_info['col']
        word = word_info['word']
        definition = word_info.get('definition', '')
        
        all_words.append({
            "id": word_id,
            "word": word.upper(),
            "definition": definition,
            "direction": "down",
            "start_row": row,
            "start_col": col,
            "length": len(word),
            "clue_number": clue_numbers[row][col],
            "alternatives": []
        })
        word_id += 1
    
    # æ„å»ºé¢„å¡«å­—æ¯ï¼ˆprefilledï¼‰
    prefilled = {}
    for (r, c) in puzzle.revealed_positions:
        if 0 <= r < grid_size and 0 <= c < grid_size and puzzle.grid[r][c]:
            prefilled[f"{r}-{c}"] = puzzle.grid[r][c]
    
    return {
        "level": level_num,
        "grid_size": grid_size,
        "cells": cells,
        "words": all_words,
        "prefilled": prefilled,
        "clue_numbers": clue_numbers,
        "density": puzzle.calculate_density(),
        "word_count": len(all_words),
        "layout_type": "dense"
    }


def generate_levels_for_group(group_code: str, vocab_info: Dict, data_dir: Path, 
                              generator: CSPPuzzleGenerator, is_primary: bool = False) -> Dict:
    """ä¸ºå•ä¸ªåˆ†ç»„ç”Ÿæˆæ‰€æœ‰å…³å¡"""
    print(f"\næ­£åœ¨ç”Ÿæˆ {group_code} ({vocab_info.get('name', '')}) çš„å…³å¡...")
    
    # åŠ è½½è¯åº“
    if isinstance(vocab_info, dict) and "book_id" in vocab_info:
        words = load_vocabulary(vocab_info["book_id"], vocab_info["category"], data_dir)
    else:
        words = []
    
    if not words:
        print(f"  æ— å¯ç”¨å•è¯ï¼Œè·³è¿‡")
        return {"levels": [], "name": vocab_info.get("name", group_code)}
    
    print(f"  åŠ è½½äº† {len(words)} ä¸ªå•è¯")
    
    # ç¡®å®šå…³å¡é…ç½®
    config = LEVEL_CONFIG["primary"] if is_primary else LEVEL_CONFIG["other"]
    
    levels = []
    level_num = 1
    
    for grid_key, count in config.items():
        grid_size = int(grid_key.replace("x", "").split("x")[0])
        print(f"  ç”Ÿæˆ {grid_key} ç½‘æ ¼ {count} å…³...")
        
        for i in range(count):
            level = generate_level(generator, words, grid_size, level_num)
            if level:
                levels.append(level)
                if level_num % 10 == 0:
                    print(f"    å·²å®Œæˆ {level_num} å…³")
            else:
                print(f"    å…³å¡ {level_num} ç”Ÿæˆå¤±è´¥")
            level_num += 1
    
    print(f"  å…±ç”Ÿæˆ {len(levels)} å…³")
    
    return {
        "name": vocab_info.get("name", group_code),
        "group_code": group_code,
        "level_count": len(levels),
        "word_count": len(words),
        "levels": levels
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("é™æ€å…³å¡æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # è·¯å¾„é…ç½®
    project_root = Path(__file__).parent.parent
    data_dir = project_root / "data"
    output_dir = project_root / "src" / "frontend" / "public" / "data" / "levels"
    test_output_dir = project_root / "data" / "test_levels"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    test_output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = CSPPuzzleGenerator()
    
    # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®
    all_campaign_data = {}
    
    # 1. ç”Ÿæˆå°å­¦å„å¹´çº§å…³å¡
    print("\n" + "=" * 40)
    print("ç”Ÿæˆå°å­¦å…³å¡")
    print("=" * 40)
    
    for grade_code, vocab_info in VOCABULARY_MAPPING["primary"].items():
        result = generate_levels_for_group(
            grade_code, vocab_info, data_dir, generator, is_primary=True
        )
        all_campaign_data[grade_code] = result
        
        # ä¿å­˜å•ä¸ªå¹´çº§æ–‡ä»¶
        output_file = output_dir / f"{grade_code}.json"
        # æ­£å¼ç‰ˆä¸å«ç­”æ¡ˆ
        public_result = create_public_version(result)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(public_result, f, ensure_ascii=False, indent=2)
        print(f"  å·²ä¿å­˜åˆ° {output_file}")
        
        # æµ‹è¯•ç‰ˆå«ç­”æ¡ˆ
        test_file = test_output_dir / f"{grade_code}_with_answers.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"  æµ‹è¯•ç‰ˆä¿å­˜åˆ° {test_file}")
    
    # 2. ç”Ÿæˆå…¶ä»–è¯åº“å…³å¡
    print("\n" + "=" * 40)
    print("ç”Ÿæˆå…¶ä»–è¯åº“å…³å¡")
    print("=" * 40)
    
    other_vocabs = {k: v for k, v in VOCABULARY_MAPPING.items() 
                    if k not in ["primary", "junior", "senior"]}
    
    for group_code, vocab_info in other_vocabs.items():
        if isinstance(vocab_info, dict) and "book_id" in vocab_info:
            result = generate_levels_for_group(
                group_code, vocab_info, data_dir, generator, is_primary=False
            )
            all_campaign_data[group_code] = result
            
            # ä¿å­˜
            output_file = output_dir / f"{group_code}.json"
            public_result = create_public_version(result)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(public_result, f, ensure_ascii=False, indent=2)
            
            test_file = test_output_dir / f"{group_code}_with_answers.json"
            with open(test_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
    
    # 3. ç”Ÿæˆç´¢å¼•æ–‡ä»¶
    print("\n" + "=" * 40)
    print("ç”Ÿæˆç´¢å¼•æ–‡ä»¶")
    print("=" * 40)
    
    index_data = {
        "version": "1.0",
        "generated_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "categories": {
            "primary": {
                "name": "å°å­¦è¯æ±‡",
                "icon": "ğŸ“š",
                "has_sub_groups": True,
                "sub_groups": [
                    {"code": code, "name": data["name"], "level_count": data["level_count"]}
                    for code, data in all_campaign_data.items()
                    if code.startswith("grade")
                ]
            },
            "exam": {
                "name": "è€ƒè¯•ç±»",
                "icon": "ğŸ“",
                "has_sub_groups": True,
                "sub_groups": [
                    {"code": "cet4", "name": "å¤§å­¦å››çº§", "level_count": all_campaign_data.get("cet4", {}).get("level_count", 0)},
                    {"code": "cet6", "name": "å¤§å­¦å…­çº§", "level_count": all_campaign_data.get("cet6", {}).get("level_count", 0)},
                    {"code": "postgrad", "name": "è€ƒç ”è¯æ±‡", "level_count": all_campaign_data.get("postgrad", {}).get("level_count", 0)},
                ]
            },
            "abroad": {
                "name": "å‡ºå›½ç•™å­¦",
                "icon": "âœˆï¸",
                "has_sub_groups": True,
                "sub_groups": [
                    {"code": "ielts", "name": "é›…æ€", "level_count": all_campaign_data.get("ielts", {}).get("level_count", 0)},
                    {"code": "toefl", "name": "æ‰˜ç¦", "level_count": all_campaign_data.get("toefl", {}).get("level_count", 0)},
                    {"code": "gre", "name": "GRE", "level_count": all_campaign_data.get("gre", {}).get("level_count", 0)},
                ]
            }
        },
        "total_levels": sum(d.get("level_count", 0) for d in all_campaign_data.values())
    }
    
    index_file = output_dir / "index.json"
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    print(f"ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜åˆ° {index_file}")
    
    # 4. ä¿å­˜å®Œæ•´æ•°æ®åˆ°åç«¯
    backend_file = project_root / "src" / "data" / "primary_campaign_levels.json"
    backend_file.parent.mkdir(parents=True, exist_ok=True)
    
    # åªä¿å­˜å°å­¦å¹´çº§æ•°æ®åˆ°åç«¯
    primary_data = {k: v for k, v in all_campaign_data.items() if k.startswith("grade")}
    with open(backend_file, 'w', encoding='utf-8') as f:
        json.dump(primary_data, f, ensure_ascii=False, indent=2)
    print(f"åç«¯æ•°æ®å·²ä¿å­˜åˆ° {backend_file}")
    
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå®Œæˆ!")
    print(f"æ€»å…³å¡æ•°: {index_data['total_levels']}")
    print("=" * 60)


def create_public_version(data: Dict) -> Dict:
    """åˆ›å»ºä¸å«ç­”æ¡ˆçš„å…¬å¼€ç‰ˆæœ¬"""
    public_data = {
        "name": data.get("name"),
        "group_code": data.get("group_code"),
        "level_count": data.get("level_count"),
        "word_count": data.get("word_count"),
        "levels": []
    }
    
    for level in data.get("levels", []):
        public_level = {
            "level": level.get("level"),
            "grid_size": level.get("grid_size"),
            "cells": level.get("cells"),  # ä¿ç•™cellsç”¨äºæ˜¾ç¤ºç½‘æ ¼ç»“æ„
            "words": [],
            "prefilled": level.get("prefilled"),
            "clue_numbers": level.get("clue_numbers"),
            "word_count": level.get("word_count"),
        }
        
        # å•è¯ä¿¡æ¯ä¸å«å®Œæ•´ç­”æ¡ˆï¼Œåªä¿ç•™å¿…è¦ä¿¡æ¯
        for word in level.get("words", []):
            public_word = {
                "id": word.get("id"),
                "word": word.get("word"),  # ä¿ç•™å•è¯ç”¨äºéªŒè¯
                "definition": word.get("definition"),
                "direction": word.get("direction"),
                "start_row": word.get("start_row"),
                "start_col": word.get("start_col"),
                "length": word.get("length"),
                "clue_number": word.get("clue_number"),
                "alternatives": word.get("alternatives", [])
            }
            public_level["words"].append(public_word)
        
        public_data["levels"].append(public_level)
    
    return public_data


if __name__ == "__main__":
    main()
